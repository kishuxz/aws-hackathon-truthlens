{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a92069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade pip\n",
    "%pip install -q boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367358b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kishorekumar/Hackathon_Project/Truthlens\n",
      "CWD: /Users/kishorekumar/Hackathon_Project/Truthlens\n",
      "Here: ['core', 'app', 'data', 'scorecards']\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import os, pathlib\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Here:\", [p.name for p in pathlib.Path('.').iterdir()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80bcbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "session = boto3.Session(profile_name=\"default\", region_name=\"us-east-2\")\n",
    "brt = session.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abfc364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is the exact response you requested:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"ok\": true\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import boto3, json\n",
    "\n",
    "# use the default profile you configured\n",
    "session = boto3.Session(profile_name=\"default\", region_name=\"us-east-2\")\n",
    "brt = session.client(\"bedrock-runtime\")\n",
    "MODEL_ID = \"us.amazon.nova-micro-v1:0\"\n",
    "\n",
    "# smoke test\n",
    "resp = brt.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=[{\"role\":\"user\",\"content\":[{\"text\": 'Return exactly {\"ok\": true}'}]}]\n",
    ")\n",
    "print(resp[\"output\"][\"message\"][\"content\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0fbc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import uuid, json\n",
    "\n",
    "SYSTEM = \"\"\"You score the reliability of a post using only the provided sources and policy.\n",
    "Independence: count sources only if organizations differ.\n",
    "Rules:\n",
    "- If 2+ independent Tier A or P support and 0 contradict → +25.\n",
    "- Any Tier A/P contradiction → −30.\n",
    "- Each Tier B support → +15; each Tier C support → +5 (cap total support at +25).\n",
    "- If total sources < 2 and no Tier P → clamp label to \"Uncertain\" (score ≤ 69).\n",
    "Return EXACTLY a JSON object with fields:\n",
    "version, score (0-100), label, contributions[], bullets[3], badges[], citations[], meta{post_id,run_id,latency_ms}.\n",
    "No prose or markdown outside the JSON. Do not invent sources.\n",
    "\"\"\"\n",
    "\n",
    "def score_one(item, policy):\n",
    "    t0 = datetime.utcnow()\n",
    "    payload = {\n",
    "        \"post_id\": item.get(\"post_id\"),\n",
    "        \"post\": item[\"post\"],\n",
    "        \"sources\": item[\"sources\"],\n",
    "        \"scoring_policy\": policy\n",
    "    }\n",
    "    response = brt.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        system=[{\"text\": SYSTEM}],\n",
    "        messages=[{\"role\":\"user\",\"content\":[{\"text\": json.dumps(payload)}]}]\n",
    "    )\n",
    "    text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    sc = json.loads(text)\n",
    "    sc.setdefault(\"version\",\"1.0\")\n",
    "    sc.setdefault(\"badges\",[])\n",
    "    sc.setdefault(\"meta\",{})\n",
    "    sc[\"meta\"][\"post_id\"] = item.get(\"post_id\")\n",
    "    sc[\"meta\"][\"run_id\"] = sc[\"meta\"].get(\"run_id\", str(uuid.uuid4()))\n",
    "    sc[\"meta\"][\"latency_ms\"] = sc[\"meta\"].get(\"latency_ms\", int((datetime.utcnow()-t0).total_seconds()*1000))\n",
    "    return sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8249e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 'p1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# if your notebook is inside Truthlens/app, ROOT is the parent\n",
    "ROOT = Path(\"..\") if Path(\".\").name == \"app\" else Path(\".\")\n",
    "\n",
    "policy = json.loads((ROOT/\"core/scoring.json\").read_text())\n",
    "samples = [json.loads(l) for l in (ROOT/\"data/postproof/sample.jsonl\").read_text().splitlines()]\n",
    "len(samples), samples[0][\"post_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2feffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x_/jm_jz0dd2_x50hv78wl2hvtc0000gn/T/ipykernel_6312/2943325850.py:17: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  t0 = datetime.utcnow()\n",
      "/var/folders/x_/jm_jz0dd2_x50hv78wl2hvtc0000gn/T/ipykernel_6312/2943325850.py:36: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  sc[\"meta\"][\"latency_ms\"] = sc[\"meta\"].get(\"latency_ms\", int((datetime.utcnow()-t0).total_seconds()*1000))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 Supported 25\n",
      "p2 Uncertain 10\n",
      "p3 Uncertain 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "outdir = (ROOT/\"scorecards\"); outdir.mkdir(exist_ok=True)\n",
    "\n",
    "results = []\n",
    "for item in samples:\n",
    "    sc = score_one(item, policy)\n",
    "    results.append(sc)\n",
    "    print(item[\"post_id\"], sc[\"label\"], sc[\"score\"])\n",
    "    (outdir/f\"{item['post_id']}.json\").write_text(json.dumps(sc, indent=2))\n",
    "\n",
    "len(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625cf5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'badges': [],\n",
      " 'bullets': ['Airport closure due to snow confirmed by AP.',\n",
      "             'City DOT confirms all flights suspended due to weather.',\n",
      "             'Travel Blog provides unrelated terminal information.'],\n",
      " 'citations': [{'source': 'AP',\n",
      "                'title': 'Airport announces closure due to snowfall.'},\n",
      "               {'source': 'City DOT',\n",
      "                'title': 'All flights suspended today due to weather.'}],\n",
      " 'contributions': [{'source': 'AP',\n",
      "                    'stance': 'support',\n",
      "                    'tier': 'A',\n",
      "                    'weight': 25},\n",
      "                   {'source': 'City DOT',\n",
      "                    'stance': 'support',\n",
      "                    'tier': 'P',\n",
      "                    'weight': 25}],\n",
      " 'label': 'Supported',\n",
      " 'meta': {'latency_ms': None, 'post_id': 'p1', 'run_id': None},\n",
      " 'score': 25,\n",
      " 'version': '1.0'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c59278b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import uuid, json\n",
    "\n",
    "def finalize_score(sc, item, policy, t0):\n",
    "    # (paste your full finalize_score here)\n",
    "    ...\n",
    "\n",
    "def score_one(item, policy):\n",
    "    # (paste your full score_one here)\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335582f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[32m     10\u001b[39m     sc = score_one(item, policy)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(item[\u001b[33m\"\u001b[39m\u001b[33mpost_id\u001b[39m\u001b[33m\"\u001b[39m], \u001b[43msc\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, sc[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     12\u001b[39m     (outdir/\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[33m'\u001b[39m\u001b[33mpost_id\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m).write_text(json.dumps(sc, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ROOT = Path(\"..\") if Path(\".\").name == \"app\" else Path(\".\")\n",
    "\n",
    "policy = json.loads((ROOT/\"core/scoring.json\").read_text())\n",
    "samples = [json.loads(l) for l in (ROOT/\"data/postproof/sample.jsonl\").read_text().splitlines()]\n",
    "\n",
    "outdir = ROOT/\"scorecards\"; outdir.mkdir(exist_ok=True)\n",
    "\n",
    "for item in samples:\n",
    "    sc = score_one(item, policy)\n",
    "    print(item[\"post_id\"], sc[\"label\"], sc[\"score\"])\n",
    "    (outdir/f\"{item['post_id']}.json\").write_text(json.dumps(sc, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81ed254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function score_one at 0x10edbb9c0>\n",
      "-----\n",
      "def score_one(item, policy):\n",
      "    # (paste your full score_one here)\n",
      "    ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(score_one)\n",
    "print(\"-----\")\n",
    "print(inspect.getsource(score_one))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87ffab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, uuid\n",
    "from datetime import datetime\n",
    "\n",
    "def finalize_score(sc, item, policy, t0):\n",
    "    \"\"\"\n",
    "    Recompute score deterministically using policy + provided sources.\n",
    "    Fills label from ladder and ensures meta.run_id / meta.latency_ms are set.\n",
    "    \"\"\"\n",
    "    base = policy.get(\"base\", 50)\n",
    "    w = policy[\"weights\"]\n",
    "\n",
    "    def T(x): return (x or \"\").upper()\n",
    "\n",
    "    # counts per our simple synthetic schema\n",
    "    sup_AP = sum(1 for s in item[\"sources\"]\n",
    "                 if s.get(\"stance\") == \"support\" and T(s.get(\"tier\")) in (\"A\", \"P\"))\n",
    "    con_AP = sum(1 for s in item[\"sources\"]\n",
    "                 if s.get(\"stance\") == \"contradict\" and T(s.get(\"tier\")) in (\"A\", \"P\"))\n",
    "    sup_B  = sum(1 for s in item[\"sources\"]\n",
    "                 if s.get(\"stance\") == \"support\" and T(s.get(\"tier\")) == \"B\")\n",
    "    sup_C  = sum(1 for s in item[\"sources\"]\n",
    "                 if s.get(\"stance\") == \"support\" and T(s.get(\"tier\")) == \"C\")\n",
    "\n",
    "    score = base\n",
    "    # contradiction from Tier A/P dominates\n",
    "    if con_AP >= 1:\n",
    "        score += w.get(\"contradicts_from_tierA_or_P>=1\", -30)\n",
    "\n",
    "    # if 2+ independent Tier A/P supports and no contradiction → add +25\n",
    "    if sup_AP >= 2 and con_AP == 0:\n",
    "        score += w.get(\"supports_from_tierA_or_P>=2\", 25)\n",
    "\n",
    "    # add lower-tier support\n",
    "    score += sup_B * w.get(\"tierB_support\", 15)\n",
    "    score += sup_C * w.get(\"tierC_support\", 5)\n",
    "\n",
    "    # clamp score to 0–100\n",
    "    score = max(0, min(100, score))\n",
    "\n",
    "    # evidence floor: if <2 sources and no Tier P, clamp label to Uncertain\n",
    "    floor = policy.get(\"evidence_floor\", {\"min_sources\": 2})\n",
    "    has_tierP = any(T(s.get(\"tier\")) == \"P\" for s in item[\"sources\"])\n",
    "    forced_label = None\n",
    "    if len(item[\"sources\"]) < floor.get(\"min_sources\", 2) and not has_tierP:\n",
    "        forced_label = \"Uncertain\"\n",
    "\n",
    "    # map numeric score to label via ladder\n",
    "    ladder = sorted(policy[\"ladder\"], key=lambda b: b[\"min\"], reverse=True)\n",
    "    label = next((b[\"label\"] for b in ladder if score >= b[\"min\"]), ladder[-1][\"label\"])\n",
    "    if forced_label:\n",
    "        label = forced_label\n",
    "\n",
    "    sc[\"score\"] = score\n",
    "    sc[\"label\"] = label\n",
    "\n",
    "    # ensure meta\n",
    "    m = sc.setdefault(\"meta\", {})\n",
    "    if not m.get(\"run_id\"):\n",
    "        m[\"run_id\"] = str(uuid.uuid4())\n",
    "    if not m.get(\"latency_ms\"):\n",
    "        m[\"latency_ms\"] = int((datetime.utcnow() - t0).total_seconds() * 1000)\n",
    "\n",
    "    return sc\n",
    "\n",
    "def score_one(item, policy):\n",
    "    \"\"\"\n",
    "    Calls Bedrock (Nova Micro) to produce an explainer JSON,\n",
    "    then normalizes numeric score/label deterministically.\n",
    "    Always returns a dict.\n",
    "    \"\"\"\n",
    "    t0 = datetime.utcnow()\n",
    "    payload = {\n",
    "        \"post_id\": item.get(\"post_id\"),\n",
    "        \"post\": item[\"post\"],\n",
    "        \"sources\": item[\"sources\"],\n",
    "        \"scoring_policy\": policy\n",
    "    }\n",
    "\n",
    "    # Ask the model\n",
    "    resp = brt.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        system=[{\"text\": SYSTEM}],\n",
    "        messages=[{\"role\":\"user\",\"content\":[{\"text\": json.dumps(payload)}]}]\n",
    "    )\n",
    "\n",
    "    # Extract text\n",
    "    text = resp[\"output\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "\n",
    "    # Parse JSON defensively\n",
    "    try:\n",
    "        sc = json.loads(text)\n",
    "        if not isinstance(sc, dict):\n",
    "            raise ValueError(\"Model returned non-dict JSON\")\n",
    "    except Exception:\n",
    "        sc = {\n",
    "            \"version\": \"1.0\",\n",
    "            \"score\": policy.get(\"base\", 50),\n",
    "            \"label\": \"Uncertain\",\n",
    "            \"contributions\": [],\n",
    "            \"bullets\": [\"Model returned non-JSON or unexpected shape.\"],\n",
    "            \"badges\": [],\n",
    "            \"citations\": [],\n",
    "            \"meta\": {}\n",
    "        }\n",
    "\n",
    "    # normalize/override numeric score + label and fill meta\n",
    "    sc = finalize_score(sc, item, policy, t0)\n",
    "    return sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caab9f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def score_one(item, policy):', '    \"\"\"', '    Calls Bedrock (Nova Micro) to produce an explainer JSON,', '    then normalizes numeric score/label deterministically.', '    Always returns a dict.']\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(score_one).splitlines()[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fa5ce9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x_/jm_jz0dd2_x50hv78wl2hvtc0000gn/T/ipykernel_6312/2519230224.py:71: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  t0 = datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> Supported 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x_/jm_jz0dd2_x50hv78wl2hvtc0000gn/T/ipykernel_6312/2519230224.py:61: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  m[\"latency_ms\"] = int((datetime.utcnow() - t0).total_seconds() * 1000)\n"
     ]
    }
   ],
   "source": [
    "test_sc = score_one(samples[0], policy)\n",
    "print(type(test_sc), test_sc.get(\"label\"), test_sc.get(\"score\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fa26622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x_/jm_jz0dd2_x50hv78wl2hvtc0000gn/T/ipykernel_6312/2519230224.py:71: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  t0 = datetime.utcnow()\n",
      "/var/folders/x_/jm_jz0dd2_x50hv78wl2hvtc0000gn/T/ipykernel_6312/2519230224.py:61: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  m[\"latency_ms\"] = int((datetime.utcnow() - t0).total_seconds() * 1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 Supported 75\n",
      "p2 Low 25\n",
      "p3 Uncertain 55\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "ROOT = Path(\"..\") if Path(\".\").name == \"app\" else Path(\".\")\n",
    "outdir = ROOT/\"scorecards\"; outdir.mkdir(exist_ok=True)\n",
    "\n",
    "for item in samples:\n",
    "    sc = score_one(item, policy)\n",
    "    print(item[\"post_id\"], sc[\"label\"], sc[\"score\"])\n",
    "    (outdir/f\"{item['post_id']}.json\").write_text(json.dumps(sc, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d69f4a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m printed\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     ROOT = Path(\u001b[34;43m__file__\u001b[39;49m).resolve().parents[\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# repo root\u001b[39;00m\n\u001b[32m    121\u001b[39m     in_jsonl = ROOT / \u001b[33m\"\u001b[39m\u001b[33mdata/postproof/sample.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    122\u001b[39m     policy   = ROOT / \u001b[33m\"\u001b[39m\u001b[33mcore/scoring.json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# app/score.py\n",
    "import os, json, uuid, sys, time\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Config ----------\n",
    "REGION   = os.getenv(\"REGION\", \"us-east-2\")\n",
    "MODEL_ID = os.getenv(\"MODEL_ID\", \"us.amazon.nova-micro-v1:0\")\n",
    "USE_BEDROCK = os.getenv(\"USE_BEDROCK\", \"1\") not in (\"0\", \"false\", \"False\")\n",
    "\n",
    "SYSTEM = \"\"\"You score the reliability of a post using only the provided sources and policy.\n",
    "Independence: count sources only if organizations differ.\n",
    "Rules:\n",
    "- If 2+ independent Tier A or P support and 0 contradict → +25.\n",
    "- Any Tier A/P contradiction → −30.\n",
    "- Each Tier B support → +15; each Tier C support → +5 (cap total support at +25).\n",
    "- If total sources < 2 and no Tier P → clamp label to \"Uncertain\" (score ≤ 69).\n",
    "Return EXACTLY a JSON object with fields:\n",
    "version, score (0-100), label, contributions[], bullets[3], badges[], citations[], meta{post_id,run_id,latency_ms}.\n",
    "No prose or markdown outside the JSON. Do not invent sources.\n",
    "\"\"\"\n",
    "\n",
    "# ---------- Deterministic overlay ----------\n",
    "def finalize_score(sc: dict, item: dict, policy: dict, t0):\n",
    "    base = policy.get(\"base\", 50)\n",
    "    w = policy[\"weights\"]\n",
    "    T = lambda x: (x or \"\").upper()\n",
    "\n",
    "    sup_AP = sum(1 for s in item[\"sources\"]\n",
    "                 if s.get(\"stance\") == \"support\" and T(s.get(\"tier\")) in (\"A\", \"P\"))\n",
    "    con_AP = sum(1 for s in item[\"sources\"]\n",
    "                 if s.get(\"stance\") == \"contradict\" and T(s.get(\"tier\")) in (\"A\", \"P\"))\n",
    "    sup_B  = sum(1 for s in item[\"sources\"]\n",
    "                 if s.get(\"stance\") == \"support\" and T(s.get(\"tier\")) == \"B\")\n",
    "    sup_C  = sum(1 for s in item[\"sources\"]\n",
    "                 if s.get(\"stance\") == \"support\" and T(s.get(\"tier\")) == \"C\")\n",
    "\n",
    "    score = base\n",
    "    if con_AP >= 1:\n",
    "        score += w.get(\"contradicts_from_tierA_or_P>=1\", -30)\n",
    "    if sup_AP >= 2 and con_AP == 0:\n",
    "        score += w.get(\"supports_from_tierA_or_P>=2\", 25)\n",
    "    score += sup_B * w.get(\"tierB_support\", 15)\n",
    "    score += sup_C * w.get(\"tierC_support\", 5)\n",
    "    score = max(0, min(100, score))\n",
    "\n",
    "    floor = policy.get(\"evidence_floor\", {\"min_sources\": 2})\n",
    "    has_tierP = any(T(s.get(\"tier\")) == \"P\" for s in item[\"sources\"])\n",
    "    forced_label = \"Uncertain\" if (len(item[\"sources\"]) < floor[\"min_sources\"] and not has_tierP) else None\n",
    "\n",
    "    ladder = sorted(policy[\"ladder\"], key=lambda b: b[\"min\"], reverse=True)\n",
    "    label = next((b[\"label\"] for b in ladder if score >= b[\"min\"]), ladder[-1][\"label\"])\n",
    "    if forced_label: label = forced_label\n",
    "\n",
    "    sc.setdefault(\"version\", \"1.0\")\n",
    "    sc.setdefault(\"badges\", [])\n",
    "    sc.setdefault(\"meta\", {})\n",
    "    sc[\"score\"] = score\n",
    "    sc[\"label\"] = label\n",
    "\n",
    "    m = sc[\"meta\"]\n",
    "    m.setdefault(\"run_id\", str(uuid.uuid4()))\n",
    "    if not m.get(\"latency_ms\"):\n",
    "        m[\"latency_ms\"] = int((datetime.now(timezone.utc) - t0).total_seconds() * 1000)\n",
    "\n",
    "    return sc\n",
    "\n",
    "# ---------- Bedrock call (optional) ----------\n",
    "def call_bedrock(payload):\n",
    "    import boto3  # lazy import\n",
    "    brt = boto3.client(\"bedrock-runtime\", region_name=REGION)\n",
    "    resp = brt.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        system=[{\"text\": SYSTEM}],\n",
    "        messages=[{\"role\":\"user\",\"content\":[{\"text\": json.dumps(payload)}]}],\n",
    "        inferenceConfig={\"maxTokens\": 400}\n",
    "    )\n",
    "    text = resp[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    return json.loads(text)\n",
    "\n",
    "# ---------- Main scorer ----------\n",
    "def score_one(item, policy):\n",
    "    t0 = datetime.now(timezone.utc)\n",
    "    payload = {\n",
    "        \"post_id\": item.get(\"post_id\"),\n",
    "        \"post\": item[\"post\"],\n",
    "        \"sources\": item[\"sources\"],\n",
    "        \"scoring_policy\": policy\n",
    "    }\n",
    "    if USE_BEDROCK:\n",
    "        try:\n",
    "            sc = call_bedrock(payload)\n",
    "        except Exception as e:\n",
    "            # fail-safe: structured fallback\n",
    "            sc = {\"bullets\": [\"(fallback) scorer unavailable\"], \"contributions\": [], \"citations\": [], \"meta\": {\"post_id\": item.get(\"post_id\")}}\n",
    "    else:\n",
    "        sc = {\"bullets\": [\"(offline) deterministic only\"], \"contributions\": [], \"citations\": [], \"meta\": {\"post_id\": item.get(\"post_id\")}}\n",
    "\n",
    "    sc = finalize_score(sc, item, policy, t0)\n",
    "    sc[\"meta\"][\"post_id\"] = item.get(\"post_id\")\n",
    "    return sc\n",
    "\n",
    "# ---------- CLI ----------\n",
    "def run_cli(in_jsonl: Path, policy_path: Path, out_dir: Path):\n",
    "    policy = json.loads(policy_path.read_text())\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    lines = [l for l in in_jsonl.read_text().splitlines() if l.strip()]\n",
    "    samples = [json.loads(l) for l in lines]\n",
    "\n",
    "    printed = 0\n",
    "    for item in samples:\n",
    "        sc = score_one(item, policy)\n",
    "        printed += 1\n",
    "        print(f\"{item['post_id']} {sc['label']} {sc['score']}\")\n",
    "        (out_dir / f\"{item['post_id']}.json\").write_text(json.dumps(sc, indent=2))\n",
    "    return printed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ROOT = Path(__file__).resolve().parents[1]  # repo root\n",
    "    in_jsonl = ROOT / \"data/postproof/sample.jsonl\"\n",
    "    policy   = ROOT / \"core/scoring.json\"\n",
    "    out_dir  = ROOT / \"scorecards\"\n",
    "\n",
    "    n = run_cli(in_jsonl, policy, out_dir)\n",
    "    print(f\"\\nWrote {n} scorecards to {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8d628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
